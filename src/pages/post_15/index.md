---
path: "/blogs/noopener"
date: "2019-08-17"
title: "Understanding NLP Pre-trained models"
author: "Olalekan taofeek"
timeToRead: "3"
smallTitle: "Natural Language processing"
description: "Understanding Natural languguage processing pretrained models"
postNum: "15"
ogimage: "./cover_15.png"
---

<img src="./cover_15.png"/>
<br/>

Language Understanding and Language translation still remains one of the complex and difficult areas hindering human Interactions. NLP(Language Language processing) still remains one of the key research focused area among the #deeplearning researchers trying to teach machines how to better understand and translate multiple languages in real time.

There are several pretrained models for language translation tasks such as #Google BERT, XLNet and the recently Baidu **EIRNE**(Enhanced Representation through Knowledge Integration) model which outperformed the counterparts having achieved the highest score which baidu emphasis continual pre-training and multi-task learning helps to achieve such an efficient score.

The future is Intriguing with such advancements in the integral parts of human life like language translation and understanding.
